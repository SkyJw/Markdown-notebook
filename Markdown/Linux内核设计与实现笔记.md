# Linux内核设计与实现笔记

## 第二章 从内核出发

### 内联函数(inline)

​	定义时，前面应该使用static，如下：

​	` static inline void xxx(unsigned int  x) `

​	常用于定义使用频繁，且代码量较小的函数。实践中，一般将内联函数定义到头文件中，<font color=red>因为使用了static限定，所以编译时，不会创建函数体。</font>如果一个内联函数仅在一个源文件中使用，那么也可以将该内联函数定义在源文件的开头处。

### 内核中不要进行浮点运算

​	内核中不应该去进行浮点运算

### 内核中容积小而固定的栈

​	用户空间可以从栈上分配大量空间来存放变量，甚至巨大的结构体或是很大的数组，因为用户空间栈大，且可以动态增长。而内核空间栈很小，与体系结构有关，一般来说32位为8K，64位为16K。

​	思考：<font color=red>按照这个说法，可以推测出两点</font>

- 在程序中声明定义变量时，是从栈空间中申请的
- 内核中定义各种结构体，都是通过定义指针，然后分配内存的方式，就是避免栈超出

## 第四章 进程调度

### 多任务

#### 抢占式多任务

​	由调度程序来决定什么时候停止一个进程的运行，以便其他进程能够得到执行机会，这个强制的挂起动作就叫抢占。进程在被抢占之前运行的时间是预先设置好的，即进程的时间片。

#### 非抢占多任务

​	在非抢占多任务模式下，除非进程自己主动停止运行，否则它会一直执行。进程主动挂起自己的操作称为让步

### Linux的进程调度

#### O(1)调度程序缺点

​	对与调度响应时间敏感的程序有先天不足，这些程序称为交互程序，如用户交互程序

### Linux调度策略

#### I/O消耗型进程

​	指进程大部分时间用来提交I/O请求或者是等待I/O请求

#### 处理器消耗型进程

​	指进程大部分时间耗费在执行代码上，除非被抢占，否则他们通常一直都不停的运行。

#### 进程优先级

- nice值
  - 范围-20到+19，默认为0，值越大优先级越低
  - unix系统的标准化概念
- 实时优先级
  - 值可以配置
  - 范围从0到99，值越大优先级越高
  - 任何实时进程优先级都高于普通进程。思考：<font color=red>实时进程和普通进程是linux中不同的两种进程类型</font>

#### 时间片

​	即进程在被抢占前所能持续运行的时间，即分配给各个进程的一次运行机会的时间长度。

​	时间片过长会导致系统对交互响应表现欠佳，时间片太短会明显增大进程切换带来的处理器耗时。

​	I/O消耗型进程不需要时间片长，因为它需要处理器运行的部分少，更多的是I/O操作，处理器消耗型进程则希望时间片越长越好，这样可以使它们的cache命中率更高。

​	<font color=red>Linux的CFS（完全公平调度器）并没有直接分配时间片到进程，而是将处理器的使用比划分给进程。这样进程所获得的处理器时间，将和系统负载密切相关。（不太明白此处如何和系统负载挂钩）</font>

​	此外分配给进程的处理器使用比也和nice值有关，nice值越低，权重越高，抢占更多的处理器使用比。

​	<font color=blue>Linux CFS调度器的抢占时机取决于新的可运行程序消耗了多少处理器使用比。如果消耗的使用比比当前进程小，则新进程立刻投入运行，抢占当前进程。否则，将推迟其运行。</font>（这句又不太懂，到底是“新的可运行程序消耗了多少处理器使用比”还是“新的可运行程序拥有多少处理器使用比”）

​	<font color=red>思考：首先这样理解处理器使用比，单位时间内，某个进程占据cpu的时间。举例说明，1秒时间内，某个进程可以占据cpu 0.5秒，则该进程使用比则为50%。这点明确后，再来看上面蓝字，“新的可运行程序”的意思应该是相较与当前运行的程序而言，而非是指的刚刚进入运行队列的程序，也就是说“新的可运行程序”应该是在运行队列中，但还没有运行的程序。这点很重要，可以解答上面括号疑问。所以这样看来，蓝字中的“抢占时机取决于新的可运行程序消耗了多少处理器使用比”这句话就明朗了，举例说明，当前运行进程分配了10%使用比，消耗了5%使用比，队列中的下个进程分配了20%使用比，而消耗了4%使用比，则此时就会发送调度。所以说CFS调度器的调度时机，不取决与分配了多少使用比，而在于消耗了多少使用比。这样也符合让所有进程都能得到运行的基本要求</font>

### Linux调度的实现

#### 4.5.1 时间记账

​	简单来说，时间记账就是记录系统中进程的运行时间。

#### 4.5.2 进程选择

​	CFS选择进程，就是选择vuntime最小的进程。如何找出vruntime最小的进程呢？首先进程队列通过红黑树进行存储，红黑树是一种以树节点形式存储的数据，这些数据都会对应一个键值，可以通过键值来快速检索节点上的数据。（时间复杂度是O(log(n))?)

##### 1. 挑选下一个任务

​	假设已经有一个存储了系统所有可运行进程的红黑树，其中节点的键值便是可运行的进程的虚拟运行时间vruntime。CFS调度就会选择该树中最左侧的叶子节点。<font color=red>这句话也就是说，红黑树最左侧叶子节点键值最小。</font>所以从红黑树的根节点出发沿着左边子节点向下查找，一直到叶子节点即可获得vruntime最小的那个进程。

​	实际使用中，其实是把最左叶子节点缓存在rb_leftmost中。

##### 2. 向树中加入进程

​	向进程的红黑树加入进程，以及缓存最左叶子节点发生在进程变为可运行状态（被唤醒）时，或者是通过fork()调用第一次创建进程时。

##### 3. 从树中删除进程

​	删除进程的动作发生在进程堵塞(变为不可运行态)或者终止时(结束运行)

#### 4.5.3 调度器入口

​	进程调度的主要入口点是函数schedule()。该函数会找到一个最高优先级的调度器类，再从该类中获取到下一个应该运行的进程。schedule()中主要还是调用pick_next_task()函数进行相关操作。

​	<font color=red>每一个调度类都实现了pick_next_task()函数，它会返回指向下一个可运行进程的指针。</font>

#### 4.5.4 睡眠和唤醒

​	休眠（被阻塞）的进程处于一个特殊的不可执行状态。若没有这种状态，休眠就必须以轮询方式实现（思考：如果不像这样使用两个队列存储休眠和运行进程，那就要用标志位来标识休眠进程，所以叫轮询实现）。

##### 1. 等待队列

​	<font color=red>等待队列的新理解：等待队列不是一种内核驱动中实现阻塞的方法，它其实更厉害，所有内核中的休眠都要依赖与等待队列，它是内核的一种基本机制，因为休眠则必须有唤醒，而等待队列正是将休眠进程和唤醒进程联系起来的机制。</font>

##### 2. 唤醒

​	唤醒通过wake_up()进行，它会唤醒指定等待队列上的所有进程。

### 4.6 抢占和上下文切换

​	上下文切换，即从一个可执行进程，切换到另一个可执行进程。由context_switch()函数负责处理。每当一个新的进程被选出来准备投入运行的时候，schedule()就会调用该函数。

​	主要完成如下两个工作：

- 调用switch_mm()，将虚拟内存从上一个进程映射到新进程
- 调用switch_to()，从上一个进程的处理器状态切换到新进程处理器状态，包括保存、恢复栈信息和寄存器信息，还有其他任何与体系结构相关的状态信息，都以进程为单位进行保存。

#### 4.6.1 用户抢占

#### 4.6.2 内核抢占

### 4.7 实时调度策略

​	Linux提供了两种实时调度策略：

- SCHED_FIFO

  一种简单的、先入先出的调度算法，不使用时间片。一个SCHED_FIFO级别的进程处于运行状态时，就会一直执行，直至它受到阻塞或者显示的释放处理器。只有更高优先级的SCHED_FIFO或者SCHED_RR任务才可以抢占SCHED_FIFO任务。

- SCHED_RR

  SCHED_RR相较于SCHED_FIFO而言，增加了时间片机制，是一种实时轮流调度算法

​	这两种实时算法实现，都是静态优先级，即内核不会实时计算优先级。Linux的实时调度是软实时，即内核会尽力去进行实时调度，但不能保证总能实时。硬实时，就是指在一定条件下，一定可以实时完成调度。

### 4.8 与调度相关的系统调用

### 4.9 小结

## 第五章 系统调用

​	系统调用是内核提供给用户进程，让其与内核进行交互的一组接口。这些接口让应用程序受限的访问硬件设备，提供了创建新进程并与已有进程进行通信的机制，也提供了申请操作系统其他资源的能力。

### 5.1 与内核通信

​		系统调用是用户进程和硬件设备之间的一个中间层。作用有三：

- 为用户空间提供硬件抽象接口
- 保证系统稳定和安全
- 使进程独立，实现每个进程都在一个虚拟系统中

### 5.2 API、POSIX和C库

​	用户空间一般不直接调用系统调用，而是调用应用编程接口（API），再由API调用系统调用。Linux最流行的API即POSIX标准。系统调用一般作为C库的一部分提供。

### 5.3 系统调用

#### 5.3.1 系统调用号

​	用于标识系统调用，用户空间传进内核空间的是系统调用号。

#### 5.3.2 系统调用性能

​	比其他系统较快，原因有二：

- linux上下文切换快
- 系统调用以及系统调用处理快

### 5.4 系统调用处理程序

​	系统调用是通过软中断实现，需要系统调用时，用户空间触发128号异常（x86），使得操作系统陷入内核态，128号异常的异常处理程序即是系统调用处理程序，名叫system_call()。

#### 5.4.1 指定恰当的系统调用

​	所有系统调用陷入内核的方式都一样，通过128号异常。如何区别各个系统调用呢？就是之前提到的系统中断号。用户空间在陷入内核前，将系统中断号存入eax寄存器，然后内核再从该寄存器取出即可。

### 5.5 系统调用的实现

#### 5.5.1 实现系统调用

#### 5.5.2 参数验证

​	内核接收一个用户空间的指针之前，内核必须保证：

- 指针指向的内存属于用户空间
- 指针指向的内存区域在该进程的地址空间
- 读、写、可执行的内存应进行对应标记

​    内核空间和用户空间之间的数据拷贝，copy_to_user(),copy_from_user()。这两个函数均有可能引起阻塞，发生在用户数据页被换出到硬盘上而不是在物理内存上的时候。

### 5.6 系统调用上下文

​	内核执行系统调用时处于进程上下文，可休眠，可抢占。

#### 5.6.1 绑定一个系统调用的最后步骤

#### 5.6.2 从用户空间访问系统调用

#### 5.6.3 为什么不通过系统调用的方式实现

#### 5.7 小结

## 第六章 内核数据结构

### 6.1 链表

​	链表是一种存放和操作可变数量元素的数据结构。链表和数组的不同之处在于，它所包含的元素都是动态创建并插入链表的，编译时不必知道具体需要创建多少个元素，而且也不需要占用连续的内存。各个元素之间通过地址指针组织到一起。

#### 6.1.1 单向链表和双向链表

#### 6.1.2 环形链表

​	Linux内核标准链表是环形双向链表。

#### 6.1.3 沿链表移动

​	沿链表移动访问，只能一个一个元素的线性访问，不能随机访问。

​	首元素有时候会用特殊的指针表示，称为头指针，该元素不用来存储数据，只用来快速定位链表头尾元素。

#### 6.1.4 Linux内核中链表的实现

##### 1. 链表数据结构

```c
	struct list_head {
		struct list_head *next;
		struct list_head *prev;
	}
```

​	linux中链表不像常规的链表那样，将数据结构体放入链表，而是先定义好链表结构，再把链表结构放入需要组织成链表的数据结构中。list_head本身并没有意义，它需要被嵌入到你自己的数据结构中才能生效。

​	<font color=red>如何通过list_head找到本节点的数据呢，这就需要container_of宏，这个宏的作用是，给定一个数据结构成员指针、该数据结构的定义和该数据结构成员名字，从而获取到指向这个数据结构的指针。该宏的原理是，一个给定结构中的变量偏移在编译时已经确定了，已知结构中的一个成员的地址，可以反推出结构的地址。</font>

​	在Linux内核中，使用list_entry()方法，封装了一下container_of()。

##### 2. 定义一个链表

​	首先，在自己的结构体中，增加一个struct list_head list成员。

```c
	struct fox {
			unsigned long tail_length;
			unsigned long weight;
			struct list_head list;
	}
```

​	链表在使用前需要初始化，因为链表的应用情景中，多数元素都是动态创建的，所以使用链表时，最常用的方法是在运行时进行初始化链表。

```c
	struct fox *red_fox;
	red_fox = kmalloc(sizeof(*red_fox), GFP_KERNEL);
	red_fox->tail_length = 40;
	red_fox->weight = 6;
	INIT_LIST_HEAD(&red_fox->list);
```

​	INIT_LIST_HEAD(ptr)宏的作用是初始化一个链表，即使一个链表的头尾指针都指向自己，具体实现如下：

```c
#define INIT_LIST_HEAD(ptr) do { \
    (ptr)->next = (ptr); (ptr)->prev = (ptr); \
} while (0)
```

​	这种方式是事先在自己的数据结构体中定义好链表list_head，然后在程序运行过程中动态的分配自己的数据结构体时，采用这个方式来动态的初始化list_head链表。

​	假如自己的数据结构体是编译期间直接定义的（全局变量方式，而不是实时分配内存），这个时候应该采用` LIST_HEAD_INIT(name) `宏，该宏实现如下：

```c
#define LIST_HEAD_INIT(name) { &(name), &(name) }
```

​	可以看出，两个宏作用均是初始化链表，使之头尾指针指向自己。区别在于一个在动态执行时使用，一个在定义时使用。<font color=red>思考：可以类比变量赋初值和赋值。</font>

##### 3. 链表头

​	按照如上的方式，已经有了一个可供使用的链表结构。不过一般还提供一个特殊的指针来索引整个链表，也就是链表头，内核提供的大多数链表相关函数都要接收这个链表头参数。构造一个链表头的方式为：static LIST_HEAD(head_name);具体实现如下：

```c
#define LIST_HEAD(name) struct list_head name = LIST_HEAD_INIT(name)
```

​	可以看出，LIST_HEAD(name)宏，其实就是定义了一个头尾指针指向自己的list_head结构体。这样可以快速定位头尾，便于插入等操作。

#### 6.1.5 操作链表

​	内核提供了一组以一个或多个list_head结构体指针做参数、复杂度为O(1)的函数来操作链表。

##### 1. 向链表中增加一个节点

```c
list_add(struct list_head *new, struct list_head *head)
```

​	该函数向指定链表的head节点后面插入new节点。<font color=red>因为链表是循环的，而且通常没有首尾节点的概念，所以你可以把任何一个节点当成head。</font>

##### 2. 从链表中删除一个节点

list_del(struct list_head *entry)

​	该函数并不会释放entry节点所包含的数据结构占用的内存

#### 6.1.6 遍历链表

##### 1. 基本方法

```c
struct list_head *p;
list_for_each(p, list) {
		/* 具体操作 */
		...
}
```

​	使用list_for_each()宏，该宏用起来和for循环差不多，小括号里面第一个参数是临时指针，类似与for循环里面的i，实时指向当前遍历到的链表节点，第二个参数是待遍历链表的链表头。

​	遍历到链表节点时，因为单独的链表节点其实没有啥用，需要的是包含链表的结构体，所以需要使用如下方法获取到结构体指针：

```c
struct list_head *p;
struct fox *f;

list_for_each(p, &fox_list) {
		/* list_entry()的三个参数，链表指针，包含链表的结构体，链表在结构体中的成员名 */
		f = list_entry(p, struct fox, list);
}
```

##### 2. 可用的方法

```c
struct fox *f;

list_for_each_entry(f, &fox_list, list) {
	/*  具体操作 */
	....
}
```

​	这种方法也就是将` list_for_each() `和` list_entry() `结合到了一起，只用填写临时指针、目标链表头、链表在数据结构体中的成员名。<font color=red>要注意的是，这里的临时指针不再是指向链表节点，而是直接指向了包含链表节点的结构体。</font>

##### 3. 反向遍历链表

```c
list_for_each_entry_reverse(pos, head, member);
```

​	与上述遍历类似，只是不再从next方向去遍历，而是从prev方向去遍历。

##### 4. 遍历的同时删除

​	标准的方法是不能一边遍历一边删除的，需要使用 以下宏：

```
list_for_each_entry_safe(pos, next, head, member);
```

​	next指针用于临时存储指向下一个节点的指针。

### 6.2 队列

​	生产者消费者的模型，先入先出。内核中的队列实现为kfifo。

#### 6.2.1 kfifo

​	kfifo维护了两个偏移变量：入口偏移和出口偏移。即kfifo的in和out。出口偏移总是小于或者等于入口偏移。

#### 6.2.2 创建队列

​	和多数内核对象一样，有动态和静态方法，常用动态方法：

```c
int kfifo_alloc(struct kfifo *fifo, unsigned int size, gfp_t gfp_mask);
```

​	静态声明更加简单：

```c
DECLARE_KFIFO(name, size);
INIT_KFIFO(name);
```

#### 6.2.3 推入队列数据

```
unsigned int kfifo_in(struct kfifo *fifo, const void *from, unsigned int len);
```

#### 6.2.4 摘取队列数据

```c
unsigned int kfifo_out(struct kfifo *fifo, void *to, unsigned int len);
```

不出队列获取数据方法：

```c
unsigned int kfifo_out_peek(struct kfifo *fifo, void *to, unsigned int len, unsigned offset);
```

#### 6.2.5 获取队列长度

​	获取kfifo队列空间总大小：

```c
static inline unsigned int kfifo_size(struct kfifo *fifo);
```

​	获取队列使用长度：

```c
static inline unsigned int kfifo_len(struct kfifo *fifo);
```

​	获取队列空闲空间：

```c
static inline unsigned int kfifo_avail(struct kfifo *fifo);
```

​	判断队列是否满/空：

```c
static inline int kfifo_is_empty(struct kfifo *fifo);
static inline int kfifo_is_full(struct kfifo *fifo);
```

#### 6.2.6 重置和撤销队列

#### 6.2.7 队列使用举例

### 6.3 映射

​	键到值的对应关系。哈希表是一种映射，但并非所有的映射都是用哈希表实现，也可以通过自平衡二叉搜索树存储数据

#### 6.3.1 初始化一个idr

#### 6.3.2 分配一个新的UID

#### 6.3.3 查找UID

#### 6.3.4 删除UID

#### 6.3.5 撤销idr

### 6.4 二叉树

#### 6.4.1 二叉搜索树

​	简称BST，是一个节点有序的树，遵循以下法则：

- 根的左分支节点值都小于根节点值
- 右分支节点值都大于根节点值
- 所有的子树也是二叉搜索树

<font color=red>	验证二叉搜索树时，不光要判断一个节点的左右子节点之间是否有序，还要判断是否满足上下界。这个上下界是由于当前节点作为上层节点的子节点，使得当前节点要满足上一层节点的成为二叉搜索树的要求而形成的。举例来说，一个根节点的左子节点下的树，其所有节点的值，都不能大于根节点。这种错误往往容易在左子节点的右子树上发生，因为紧紧考虑的左子节点的右子树节点应该大于左子节点，而忘了还应该小于根节点。</font>

#### 6.4.2 自平衡二叉搜索树

​	平衡二叉树是指所有的叶子节点的深度相差不超过1的二叉搜索树。自平衡二叉搜索树是指其所有操作都识图维持(半)平衡的二叉搜索树。<font color=blue>什么是半平衡？不明白</font>

##### 1. 红黑树

​	红黑树是一种自平衡二叉搜索树。

​	红黑色特点：

- 所有节点要么着红色，要么着黑色
- 叶子节点都是黑色
- 叶子节点不包含数据
- 所有非叶子节点都有两个子节点
- 如果一个节点是红色，则它的子节点都是黑色
- 在一个节点到其叶子节点的路径中，如果总是包含同样数目的黑色节点，则该路径相比其他路径

## 第七章 中断和中断处理

### 7.1 中断

### 7.2 中断处理函数

​	运行于中断上下文，不可阻塞，尽可能短

### 7.3 上半部和下半部对比

​	分上下部是为了解决中断处理程序要运行快，又要完成的工作量多的矛盾。

### 7.4 注册中断处理程序

​	设备驱动中使用request_irq()函数注册一个中断处理程序。函数原型如下：

```c
int request_irq(unsigned int irq, 
								irq_handler_t handler,
								unsigned long flags,
								const char *name,
								void *dev)
```

#### 7.4.1 中断处理程序标志

​	request_irq()第三个参数为中断标志

-  0 无特殊操作
- IRQF_DISABLED 执行本中断处理程序时，禁止其他所有中断
- IRQF_SAMPLE_RANDOM 该设备的中断间隔时间会作为墒填充到内核熵池，从而影响随机数的产生，类似随机数种子的意思
- IRQF_TIMER 标志着该中断特别为系统定时器的中断处理而准备的
- IRQF_SHARED 标志着多个中断处理程序可以共享一个中断线，也就是中断号

​    request_irq()第四个参数为中断名字的ascii文本表示，会在` /proc/irq `和` /proc/interrupts `中展现

​	第五个参数dev本应用于共享中断线，实践操作中，内核往往通过它传递驱动程序的设备结构

​	<font color=red>注意request_irq()函数可能会睡眠，不能在中断上下文，或不可阻塞处调用</font>

#### 7.4.2 一个中断例子

#### 7.4.3 释放中断处理程序

```c
void free_irq(unsigned int irq, void *dev)
```

### 7.5 编写中断处理程序

​	中断处理程序声明如下：

```c
static irqreturn_t intr_handler(int irq, void *dev)
```

​	第一个参数为这个中断处理程序所响应的中断号，第二个函数常用来传递设备的私有结构体

​	中断处理程序的返回值是一个特殊类型：irqreturn_t。中断处理程序可能返回两个特殊的值：IRQ_NONE、IRQ_HANDLED。如果中断处理程序检测到一个中断，但是该次中断并非在注册处理函数期间指定的产生源时，返回IRQ_NONE。当中断处理程序正确被调用，而且确实产生了期望中的中断时，返回IRQ_HANDLED。另外也可以用宏IRQ_REVAL(val)来返回，val为非零，实际返回IRQ_HANDLED，否则返回IRQ_NONE

​	Linux的中断处理程序是无须重入的，也就是说当某一中断线上的中断发生时，该中断线对应的中断会在所有处理器上屏蔽，避免了自身中断打断自身

#### 7.5.1 共享的中断处理程序

​	中断标志应设置为IRQF_SHARED，dev参数不可为空，一般为设备结构体。内核接收到一个中断后，会调用该共享中断上的所有注册的中断处理函数，所以每个中断处理函数中要判断自身设备是否真的发生了中断。例如pci中断就是共享中断

#### 7.5.2 中断处理程序实例

### 7.6 中断上下文

​	原文“中断上下文不可睡眠，否则又怎能在对它重新调度呢？”意思也就是说，中断上下文不存在任何一个阻塞队列中。正好印证了，睡眠-唤醒的过程依赖于某个等待队列

​	中断处理程序栈共享其所中断的进程的栈，32位系统是8K，64位系统是16K。所以中断处理程序从栈中获取空间时，务必非常节约。（<font color=red>问题：哪些操作会从栈上分配空间？</font>应该有全局变量/局部变量等）

### 7.7 中断处理机制的实现

### 7.8 /proc/interrupts

​	/proc/interrupts存放系统中中断相关信息，第一列是中断号，第二列是中断次数，第三列是中断控制器，最后一列是request_irq()中提供的名字

### 7.9 禁止和激活中断

#### 7.9.1 禁止和激活中断

​	保存中断系统状态的方式： 

~~~ c
unsigned long flags;
local_irq_save(flags); /* 禁止中断 */
local_irq_restore(flags); /* 中断恢复至原来的状态 */
~~~

#### 7.9.2 禁止指定中断线

```c
void disable_irq(unsigned int irq);
void disable_irq_nosync(unsigned int irq);
void enable_irq(unsigned int irq);
void syncchronize_irq(unsigned int irq);
```

​	disable_irq()禁止指定中断时，若指定中断正在执行，则disable_irq()会等待中断处理程序执行完成后返回，而disable_irq_nosync()会立即返回。

​	syncchronize_irq()用于等待一个特定的中断处理程序完成。

​	每次调用disable的函数，都需要对应的调用一次enable，才能使能。比如，连续调用两次disable_irq()，则需要调用两次enable_irq()才能够激活对应的中断线。

#### 7.9.3 中断系统的状态

​	在<asm/system.h>中定义的irqs_disable宏，可以用来判断当前处理器的中断系统状态。若中断系统被禁止，则返回非0，否则返回0

​	在<linux/hardirq.h>中定义了两个宏，可以用来检查内核的当前上下文接口，它们是

```c
in_interrupt()
in_irq()
```

​	第一个宏最有用，当内核处于任何类型的中断处理中，它返回非0，说明内核此刻正在执行中断处理程序，或者正在执行下半部处理程序。宏in_irq()只有在内核确实正在执行中断处理程序时才返回非0

​	<font color=red> 通常在程序中，当你要检查自己是否处于进程上下文中的时候（书中说此情况很常见），因为代码要做一些像睡眠这样只能从进程上下文中做的事情，可以使用in_interrupt()宏，当其返回0时，则此刻的内核处于进程上下文中</font>

### 7.10 小结

## 第八章 下半部和推后执行的工作

​	中断处理程序的局限：

- 中断处理程序是异步执行的，即不可预见其发生的时间。所以它可能会打断其他重要代码，甚至其他中断处理程序。因此，为了避免被打断的代码停止时间过长，中断处理程序应该执行得越快快好
- 有很高的时限要求
- 中断处理程序不在进程上下文中，所以它们不能阻塞，这限制了它们所做的事情

### 8.1 下半部

-  如果一个任务对时间非常敏感，将其放在中断处理程序中执行

- 如果一个任务和硬件相关，将其放在中断处理程序中执行

- 如果一个任务要保证不被其他中断（特别是相同的中断）打断，将其放在中断处理程序中执行

- 其他所有任务，考虑放置在下半部执行

  总而言之，中断处理程序执行的越快越好

#### 8.1.1 为什么要用下半部

​	当中断处理程序运行时，当前中断线所在的处理器上，会屏蔽自身中断线，甚至IRQF_DISABLED类型中断会屏蔽所有中断。使得系统的响应能力和性能造成负面影响

#### 8.1.2 下半部的环境

##### 1. "下半部"的起源

​	即”bottom half“，已经弃用

##### 2.任务队列

​	内核定义一组队列，队列中时一系列等待调用的函数组成的链表。根据其所处于队列中的位置，这些函数将在某一时刻执行。驱动程序可以将它们自己的下半部注册到合适的队列当中。这种机制表现不错，但是还是不够灵活，对性能要求较高的子系统，像网络部分，它也不能胜任。后来被工作队列取代

##### 3.软中断和tasklet

​	软中断时一组静态定义的下半部接口，有32个，可以在所有的处理器上同时执行——即使是相同类型（也就是说多个处理器上可以同时调用同一个下半部程序？）

​	tasklet名称起的很糟糕，令人费解。tasklet是基于软中断实现的，灵活性强，动态创建的下半部实现机制。两个不同类型的tasklet可以在不同的处理器上同时执行，但类型相同的tasklet不能同时执行（<font color=red>也就是说，tasklet的性能是弱于软中断</font>）。tasklet是性能和易用性之间的平衡产物。对于大部分下半部处理来说，用tasklet就足够了，除非是像网络这样对性能要求非常高的情况才需要直接使用软中断。软中断需要静态注册。（<font color=red>这里的软中断，和之前第四章系统调用提到的软中断不是一个概念，第四章的软中断是指的由软件引发的中断，准确的应该叫软件中断</font>）

​	总的来说，2.6内核版本中，提供了软中断，tasklets和工作队列三种不同形式的下半部实现机制

​	内核定时器，是将操作推迟到某个确定的时间段进行执行

### 8.2 软中断

#### 8.2.1 软中断的实现

​	软中断的数据结构：

```c
struct softirq_action {
		void (*action)(struct softirq_action *);
};
```

​	它定义在<linux/interrupt.h>中。软中断是在编译期间静态分配的，它不像tasklet那样能够被动态的注册或注销。<font color=red>这句话的意思，直白点说，就是软中断得要直接定义成变量的形式，类似于int i = 0;这种方式</font>

​	在kernel/softirq.c中定义了一个包含有32个该结构体的数组：

```c
static struct softirq_action softirq_vec[NR_SOFTIRQS];
```

​	每个被注册的软中断都占据该数组的一项，因此最多可能有32个软中断。这是固定的。在2.6内核中，这32个项只能用到9个

##### 1. 软中断处理程序

​	软中断处理程序action的函数原型：

```c
void softirq_handler(struct softirq_action *);
```

​	当内核运行一个软中断处理程序时，就会去调用action函数，其唯一参数为指向softirq_action的结构体。这种方法，可以保证以后对所有的软中断处理程序加新的参数时，也就是在softirq_action结构体中增加域，不需要改变软中断处理程序。

​	软中断不会抢占另一个软中断，唯一可以抢占软中断的是中断处理程序，也就是上半部。不过其他的软中断（甚至是同类型），可以同时在多个处理器同时运行

##### 2. 执行软中断

​	一个静态注册过的软中断，必须标记后才会执行，这被称作触发软中断。使用软中断的中断处理，会在上半部结束前，标志它对应的软中断，便于软中断在稍后执行。下列地方，待处理的软中断会被检查和执行

- 从一个硬件中断代码处返回

- 在ksoftirqd内核线程中

- 在显式检查和执行待处理的软中断的代码中

​	无论是上述哪种情况唤起，软中断都是在do_softirq()函数中执行，该函数中，获取软中断标记值，用32bit数据表示，每一个bit对应一个软中断。使用do ... while(pending >> 1)方法检查软中断是否标记，并调用对应的action函数进行执行

#### 8.2.2 使用软中断

### 8.3 tasklet

#### 8.3.1 tasklet的实现

​	tasklet由两类软中断代表：HI_SOFTIRQ/TASKLET_SOFTIRQ。两者唯一实际区别在于，HI_SOFTIRQ类型的软中断先于TASKLET_SOFTIRQ类型的软中断执行

##### 1. tasklet结构体

​	定义在<linux/interrupt.h>

```c
struct tasklet_struct {
		struct tasklet_struct *next; /* 链表中的下一个tasklet */
		unsigned long state;			  /* tasklet的状态 */
		atomic_t count;						  /* 引用计数器 */
		void (*func)(unsigned long); /* tasklet处理函数 */
		unsigned long data;				  /* 给tasklet处理函数的参数 */
};
```

state只有以下三种之一：

- 0

- TASKLET_STATE_SCHED

  已被调度，正准备投入运行

- TASKLET_STATE_RUN

  正在运行

​	count成员是tasklet的引用计数器，不为0，则tasklet被禁止

##### 2. 调度tasklet

​	已调度的tasklet（也就是已被上半部触发的tasklet），存放在两个单处理器数据结构tasklet_vec(对应普通tasklet)，tasklet_hi_vec(高优先级的tasklet)。这两个结构都是tasklet_struct组成的链表

​	tasklet调度函数：tasklet_schedule()和tasklet_hi_schedule()，下面分析tasklet_schedule()的执行步骤：

(1)  检查tasklet的状态是否为TASKLET_STATE_SCHED，如果是说明tasklet已经被调度（可能是被上半部触发过，但还没来得及执行），函数立即返回

(2) 调用_tasklet_schedule()

(3) 保存中断状态，然后禁止本地中断。

(4) 把需要调度的tasklet加到每个处理器每一个tasklet_vec链表或tasklet_hi_vec链表的表头上去

(5) 触发TASKLET_SOFTIRQ或HI_SOFTLRQ软中断，这样下次软中断的do_softirq()函数中，就会执行对应的软中断处理函数

(6) 恢复中断到原状态并返回

<font color=red>	为什么说tasklet是利用软中断实现的，这里就很好的说明了，软中断调度函数只是将tasklet放入到tasklet vec中，然后去触发软中断对应的标志，然后等待tasklet软中断的action函数中，去执行具体的tasklet任务</font>

​	软中断中处理tasklet的action函数为tasklet_action()和tasklet_hi_action()对应两种tasklet，这两个action函数的主要做了如下操作：

(1) 禁止中断，然后检索tasklet_vec或tasklet_hi_vec链表（也就是拷贝一份）

(2) 将当前处理器上的该链表设置为NULL，达到清空的效果

(3) 允许中断(<font color=red>思考：这里关闭中断的原因应该和软中断中获取软中断映射表时要关闭中断一样，是为了避免获取链表和清除链表直接发生中断，从而导致清理了不应该清理的中断，导致应该执行的tasklet不能得到执行。这样分析的话还有一个隐藏的推断，就是禁止中断不会使得实际的硬件中断丢失，而是在下次开启中断后再发送给系统，要不然这样就没有意义</font>)

(4) 循环遍历获得链表上的每一个待处理的tasklet

(5) 如果是多处理器，通过检查TASKLET_STATE_RUN来判断这个tasklet是否正在其他处理器上运行。如果它正在运行，那就不要执行，跳到下一个待处理的tasklet(前面有说过，相同的tasklet不能在不同的处理器上同时运行)

(6) 如果当前的tasklet没有执行，将其状态设置为TASKLET_STATE_RUN，避免其他处理器执行

(7) 检查count是否为0，确保tasklet没有被禁止，0为没有被禁止

(8) 执行tasklet处理程序

(9) 清除tasklet的TASKLET_STATE_RUN状态标志

(10) 重复执行下一个tasklet，直至没有剩余的等待处理的tasklet

#### 8.3.2 使用tasklet

##### 1. 声明你自己的tasklet

​	tasklet可以静态或者动态的创建，取决于对tasklet是直接引用还是间接引用

(1) 静态创建

```c
DECLARE_TASKLET(name, func, data)
DECLARE_TASKLET_DISABLED(name, func, data);
```

​	这两个宏，都可以静态的创建一个名叫name的tasklet，func对应tasklet处理函数，data为它的参数。两个宏区别在于，第一个定义之后tasklet处于激活状态，第二个会把count引用计数器设置为1，所以该tasklet处于禁止状态

​	举个例子：

```c
DECLARE_TASKLET(my_tasklet, my_tasklet_handler, dev);
```

​	这行代码等价于：

```c
struct tasklet_struct my_tasklet = {NULL, 0, ATOMIC_INIT(0), my_tasklet_handler, dev};
```

(2) 动态创建

​	这里书上讲的不太清楚，间接引用也就是用指针，用法如下：

```c
tasklet_init(t, tasklet_handler, dev);
```

​	应该t就是创建的tasklet指针

##### 2. 编写你自己的tasklet处理程序

​	tasklet处理程序有固定的形式，如下：

```c
void tasklet_handler(unsigned long data)
```

​	tasklet处理程序不能睡眠，即不能用信号量或其他会造成阻塞的函数。tasklet处理函数中，允许响应中断，所以必须做好预防工作（如屏蔽中断，然后获取一个锁）

##### 3. 调度你自己的tasklet

​	tasklet的调度，也就是触发或者说标记，就是告诉内核这个tasklet要运行。类似于触发中断，然后执行中断处理程序。tasklet的调度是使用显式的调度函数tasklet_schedule()，参数是想要运行的tasklet_struct指针。作为一种优化措施，一个tasklet总是在调度它的处理器上执行，这样是希望能更好的利用处理器的高速缓存。

##### 4. ksoftirqd

#### 8.3.3 老的BH机制

### 8.4 工作队列

​	工作队列是在进程上下文执行，所以工作队列执行的代码能够占尽进程上下文的所有优势，最重要的是工作队列允许重新调度甚至是睡眠(软中断和tasklet不可以)。

#### 8.4.1 工作队列的实现

​	工作队列其实是在内核中创建一个工作者线程，将在中断中需要推迟执行的工作，放到这个线程中进行。可以在驱动中创建这个线程，但内核中有一个缺省的工作队列进程。

​	缺省的工作者线程叫做events/n，其中n是处理器编号，也就是说，每个处理器上都有一个这样的线程。驱动可以使用缺省线程，也可以自己创建线程。自己创建线程，可能性能会好一些

##### 1. 表示线程的数据结构

​	工作者线程用workqueue_struct结构来表示：

```c
struct workqueue_struct {
	struct cpu_workqueue_struct cpu_wq[NR_CPUS];
	struct list_head list;
	const char *name;
	int sinqlethread;
	int freezeable;
	int rt;
};
```

​	结构中的数组cpu_wq中的每一项对应一个处理器。一台多核电脑，中的每一个工作者线程都会对应一个这样的cpu_workqueue_struct结构体，这个结构体是工作队列的核心数据结构：

```c
struct cpu_workqueue_struct {
	spinlock_t lock; /* 保护此结构的锁 */
	
	struct list_head worklist; /* 工作列表 */
	wait_queue_head_t more_work; /* 等待队列 */
	struct work_struct *current_struct; /* 当前工作 */
	
	struct workqueue_struct *wq; /* 关联工作队列结构 */
	task_t *thread; /* 关联线程 */
};
```

​	<font color=blue>自己理解（不一定对）：内核中只有一个workqueue_struct结构，计算机中有几个核就有几个工作者线程，每个工作者线程就对应一个cpu_workqueue_struct结构，所有的这些cpu_workqueue_struct结构都会以数组形式存放在workqueue_struct中</font>

​	<font color=red>更正：不止有一个workqueue_struct结构，内核默认有一个，驱动中可以创建自己的workqueue_struct</font>



##### 2. 表示工作的数据结构

​	所有的工作者线程都是普通的内核线程，在其中需要执行worker_thread()函数。这个函数初始化完成后，就会休眠，只到有工作插入到这个工作队列中，就被唤醒完成工作后继续休眠，周而复始。

​	工作用<linux/workqueue.h>中定义的work_struct结构体表示：

```c
struct work_struct {
	atomic_long_t data;
	struct list_head entry;
	work_func_t func;
};
```

​	这些个结构体被连成链表，在每个处理器上的工作队列都有这么一个链表。当一个工作者线程被唤醒时，它就会执行链表上的所有工作。工作执行完毕时，工作就会从该链表移去。链表空时，工作者线程就会休眠

​	worker_thread()函数的核心流程简化如下：

```c
for (;;) {
    /* 先准备休眠，将自己加入等待队列，state设置为TASK_INTERRUPTIBLE */
	prepare_to_wait(&cwq->more_work, &wait, TASK_INTERRUPTIBLE);
    
    /* 判断工作链表是否为空，为空则显示进行调度 */
	if (list_empty(&cwq->worklist)) 
		schedule();
    
    /* 有待进行的工作，结束休眠 */
	finish_wait(&cwq->more_work, &wait);
    
    /* 进行工作 */
	run_workqueue(cwq);
}
```

​	接下来由run_workqueue()函数来实际完成推后到此的工作：

```c
while (!list_empty(&cwq->worklist)) {
	struct work_struct *work;
	work_func_t f;
	void *data;
	
	work = list_entry(cwq->worklist.next, struct work_struct, entry);
	f = work->func;
	list_del_init(cwq->worklist.next);
	work_clear_pending(work); /* 清理待处理标志位 */
	f(work); /* 这里是错印吗，应该是data？或者还是之前的那种思想，便于添加额外参数？ */
}
```

##### 3. 对工作进行调度

- 使用缺省的内核工作队列线程

```c
	schedule_work(&work);
```

- 延迟delay时钟节拍执行

```c
	schedule_delayed_work(&work, delay);
```

##### 4. 刷新操作

​	在执行上述工作调度函数后，实际工作是在工作线程下一次唤醒时执行的。也就是说调度工作语句后面的代码时，工作可能尚未进行。有时候后面的代码需要工作执行后再继续，这就需要刷新操作，如下

```
	void flush_scheduled_work(void);
```

​	该函数会一直等待，直到工作队列中的所有对象都被执行后才返回，该函数会休眠

##### 5. 创建新的工作队列

​	创建新的工作队列会在每个处理器上都创建一个工作者线程，操作如下：

```
struct workqueue_struct *create_workqueue(const char *name);
```

​	创建好工作队列后，将工作放入其中不在使用3.中提到的函数，而是以下函数，区别在于下面的函数可以指定工作队列：

```
int queue_work(struct workqueue_struct * wq, struct work_struct *work);

int queue_delayed_work(struct workqueue_struct *wq,
					   struct work_struct *work,
					   unsigned long delay);
```

​	刷新指定工作队列：

```
flush_workqueue(struct workqueue_struct *wq);
```

#### 8.4.3 老的任务队列机制

### 8.5 下半部机制的选择

​	2.6内核版本有三种机制：软中断,tasklet,工作队列

​	软中断性能最高，但是不好操作，序列化保障少，可以在多个处理器同时处理同一个软中断，不可睡眠

​	tasklet基于软中断实现，比软中断好操作，体现在接口简单，不同处理器不能同时处理同一个tasklet，性能比软中断低一些，不可睡眠

​	工作队列运行于进程上下班，可以睡眠，工作队列开销最大，因为有内核线程甚至是上下文的切换

### 8.6 在下半部之间加锁

### 8.7 禁止下半部

### 8.8 小节

## 第九章 内核同步介绍

### 9.1 临界区和竞争条件

​	临界区就是访问和操作共享数据的代码段。多个执行线程并发访问同一个资源通常是不安全的，为了避免临界区的并发访问，我们必须保证这些代码原子的执行

​	如果两个线程处于同一个临界区中执行，我们称之为竞争条件。避免并发和防止竞争条件称为同步

#### 9.1.1 为什么我们需要保护

#### 9.1.2 单个变量

### 9.2 加锁

​	将临界区放入一个房间中，当有一个线程进入房间中对临界区进行操作时，把门锁上，其他线程在门口等着，直到前一个线程出来（有点像上厕所）

​	如果用常规操作实现的锁，如标志位等，实际上并没有真的解决竞争问题，因为访问标志位的操作不是不可再分的，不是原子的。所以这把锁要交由体系结构提供的原子操作指令来实现才是有效的